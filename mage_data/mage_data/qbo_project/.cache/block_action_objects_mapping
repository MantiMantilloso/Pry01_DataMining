{"block_file": {"data_exporters/export_customers.py:data_exporter:python:export customers": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom sqlalchemy import create_engine, text\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_data_to_postgres(df: pd.DataFrame, **kwargs):\n    user = get_secret_value('POSTGRES_USER')\n    password = get_secret_value('POSTGRES_PASSWORD')\n    host = get_secret_value('POSTGRES_HOST')\n    db = get_secret_value('POSTGRES_DB')\n    \n    conn_str = f'postgresql://{user}:{password}@{host}:5432/{db}'\n    engine = create_engine(conn_str)\n\n    with engine.connect() as connection:\n        trans = connection.begin()\n        try:\n            connection.execute(text(\"CREATE SCHEMA IF NOT EXISTS raw;\"))\n            \n            # --- CAMBIO CLAVE: Nombre de tabla qb_customers ---\n            create_table_sql = \"\"\"\n            CREATE TABLE IF NOT EXISTS raw.qb_customers (\n                id VARCHAR(50) PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP,\n                extract_window_start_utc VARCHAR(50),\n                extract_window_end_utc VARCHAR(50),\n                page_number INTEGER\n            );\n            \"\"\"\n            connection.execute(text(create_table_sql))\n            \n            upsert_sql = text(\"\"\"\n            INSERT INTO raw.qb_customers (id, payload, ingested_at_utc, extract_window_start_utc, extract_window_end_utc, page_number)\n            VALUES (:id, :payload, :ingested_at_utc, :extract_window_start_utc, :extract_window_end_utc, :page_number)\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number;\n            \"\"\")\n            \n            data_to_insert = df.to_dict(orient='records')\n            connection.execute(upsert_sql, data_to_insert)\n            \n            trans.commit()\n            print(f\"\u00c9xito: {len(df)} customers procesados en raw.qb_customers\")\n            \n        except Exception as e:\n            trans.rollback()\n            raise e", "file_path": "data_exporters/export_customers.py", "language": "python", "type": "data_exporter", "uuid": "export_customers"}, "data_exporters/export_invoices.py:data_exporter:python:export invoices": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom sqlalchemy import create_engine, text\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: pd.DataFrame, **kwargs):\n    \"\"\"\n    Exporta datos a Postgres esquema 'raw' con l\u00f3gica de Upsert (Idempotencia).\n    \"\"\"\n    # 1. Recuperar credenciales seguras\n    user = get_secret_value('POSTGRES_USER')\n    password = get_secret_value('POSTGRES_PASSWORD')\n    host = get_secret_value('POSTGRES_HOST')\n    db = get_secret_value('POSTGRES_DB')\n    \n    # 2. Crear conexi\u00f3n (SQLAlchemy)\n    conn_str = f'postgresql://{user}:{password}@{host}:5432/{db}'\n    engine = create_engine(conn_str)\n\n    with engine.connect() as connection:\n        trans = connection.begin()\n        try:\n            # 3. Asegurar que existe el esquema RAW\n            connection.execute(text(\"CREATE SCHEMA IF NOT EXISTS raw;\"))\n            \n            # 4. Crear tabla si no existe (con JSONB para el payload)\n            # Definimos 'id' como PRIMARY KEY para poder detectar duplicados\n            create_table_sql = \"\"\"\n            CREATE TABLE IF NOT EXISTS raw.qb_invoices (\n                id VARCHAR(50) PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP,\n                extract_window_start_utc VARCHAR(50),\n                extract_window_end_utc VARCHAR(50),\n                page_number INTEGER\n            );\n            \"\"\"\n            connection.execute(text(create_table_sql))\n            \n            # 5. Ejecutar UPSERT (Insert on Conflict Update)\n            # Esto cumple el requisito de idempotencia: si el ID ya existe, actualiza los datos.\n            upsert_sql = text(\"\"\"\n            INSERT INTO raw.qb_invoices (id, payload, ingested_at_utc, extract_window_start_utc, extract_window_end_utc, page_number)\n            VALUES (:id, :payload, :ingested_at_utc, :extract_window_start_utc, :extract_window_end_utc, :page_number)\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number;\n            \"\"\")\n            \n            # Convertimos el DF a diccionario para pasarlo a SQLAlchemy\n            data_to_insert = df.to_dict(orient='records')\n            \n            # Ejecuci\u00f3n masiva\n            connection.execute(upsert_sql, data_to_insert)\n            \n            trans.commit()\n            print(f\"\u00c9xito: {len(df)} registros procesados (Upsert) en raw.qb_invoices\")\n            \n        except Exception as e:\n            trans.rollback()\n            raise e", "file_path": "data_exporters/export_invoices.py", "language": "python", "type": "data_exporter", "uuid": "export_invoices"}, "data_exporters/export_items.py:data_exporter:python:export items": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom sqlalchemy import create_engine, text\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_data_to_postgres(df: pd.DataFrame, **kwargs):\n    user = get_secret_value('POSTGRES_USER')\n    password = get_secret_value('POSTGRES_PASSWORD')\n    host = get_secret_value('POSTGRES_HOST')\n    db = get_secret_value('POSTGRES_DB')\n    \n    conn_str = f'postgresql://{user}:{password}@{host}:5432/{db}'\n    engine = create_engine(conn_str)\n\n    with engine.connect() as connection:\n        trans = connection.begin()\n        try:\n            connection.execute(text(\"CREATE SCHEMA IF NOT EXISTS raw;\"))\n            \n            # --- CAMBIO CLAVE: Nombre de tabla qb_items ---\n            create_table_sql = \"\"\"\n            CREATE TABLE IF NOT EXISTS raw.qb_items (\n                id VARCHAR(50) PRIMARY KEY,\n                payload JSONB,\n                ingested_at_utc TIMESTAMP,\n                extract_window_start_utc VARCHAR(50),\n                extract_window_end_utc VARCHAR(50),\n                page_number INTEGER\n            );\n            \"\"\"\n            connection.execute(text(create_table_sql))\n            \n            upsert_sql = text(\"\"\"\n            INSERT INTO raw.qb_items (id, payload, ingested_at_utc, extract_window_start_utc, extract_window_end_utc, page_number)\n            VALUES (:id, :payload, :ingested_at_utc, :extract_window_start_utc, :extract_window_end_utc, :page_number)\n            ON CONFLICT (id) DO UPDATE SET\n                payload = EXCLUDED.payload,\n                ingested_at_utc = EXCLUDED.ingested_at_utc,\n                extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n                extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n                page_number = EXCLUDED.page_number;\n            \"\"\")\n            \n            data_to_insert = df.to_dict(orient='records')\n            connection.execute(upsert_sql, data_to_insert)\n            \n            trans.commit()\n            print(f\"\u00c9xito: {len(df)} items procesados en raw.qb_items\")\n            \n        except Exception as e:\n            trans.rollback()\n            raise e", "file_path": "data_exporters/export_items.py", "language": "python", "type": "data_exporter", "uuid": "export_items"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/extract_customers.py:data_loader:python:extract customers": {"content": "import io\nimport pandas as pd\nimport requests\nimport json\nimport base64\nimport time\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\ndef get_new_access_token():\n    # --- LOGICA DE ROTACION Y AUTH (IGUAL QUE ANTES) ---\n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n\n    auth_header = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n\n    headers = {\n        'Accept': 'application/json',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Authorization': f'Basic {auth_header}'\n    }\n    \n    data = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    \n    response = requests.post(url, headers=headers, data=data)\n    \n    if response.status_code != 200:\n        raise Exception(f\"Error renovando token: {response.text}\")\n        \n    tokens_dict = response.json()\n    new_refresh_token = tokens_dict.get('refresh_token')\n    \n    # Chequeo de rotaci\u00f3n\n    if new_refresh_token and new_refresh_token != refresh_token:\n        print(f\"\u26a0\ufe0f AVISO DE ROTACI\u00d3N: Nuevo Refresh Token detectado: {new_refresh_token}\")\n    \n    return tokens_dict['access_token']\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Extrae Customers de QBO.\n    \"\"\"\n    access_token = get_new_access_token()\n    realm_id = get_secret_value('QBO_REALM_ID')\n    base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}/query\"\n    \n    start_date = kwargs['fecha_inicio']\n    end_date = kwargs['fecha_fin']\n    \n    print(f\"--- Extrayendo CUSTOMERS desde {start_date} hasta {end_date} ---\")\n\n    all_data = []\n    start_position = 1\n    max_results = 100\n    has_more_data = True\n    \n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'application/text'\n    }\n\n    while has_more_data:\n        # --- CAMBIO CLAVE: Query a la tabla Customer ---\n        query = (\n            f\"SELECT * FROM Customer \"\n            f\"WHERE MetaData.LastUpdatedTime >= '{start_date}T00:00:00-05:00' \"\n            f\"AND MetaData.LastUpdatedTime <= '{end_date}T23:59:59-05:00' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n        )\n        \n        response = requests.post(base_url, headers=headers, data=query)\n        \n        if response.status_code == 429:\n            print(\"Rate Limit. Esperando 5s...\")\n            time.sleep(5)\n            continue\n            \n        if response.status_code != 200:\n            raise Exception(f\"Error API: {response.text}\")\n            \n        data = response.json()\n        query_response = data.get('QueryResponse', {})\n        # --- CAMBIO CLAVE: La llave ahora es 'Customer' ---\n        records = query_response.get('Customer', [])\n        \n        if not records:\n            has_more_data = False\n        else:\n            print(f\"Pagina extraida: {len(records)} customers.\")\n            \n            for item in records:\n                row = {\n                    'id': item['Id'],\n                    'payload': json.dumps(item),\n                    'ingested_at_utc': datetime.utcnow().isoformat(),\n                    'extract_window_start_utc': start_date,\n                    'extract_window_end_utc': end_date,\n                    'page_number': (start_position // max_results) + 1\n                }\n                all_data.append(row)\n            \n            start_position += len(records)\n            if len(records) < max_results:\n                has_more_data = False\n\n    print(f\"Total Customers: {len(all_data)}\")\n    return pd.DataFrame(all_data)", "file_path": "data_loaders/extract_customers.py", "language": "python", "type": "data_loader", "uuid": "extract_customers"}, "data_loaders/extract_invoices.py:data_loader:python:extract invoices": {"content": "import io\nimport pandas as pd\nimport requests\nimport json\nimport base64\nimport time\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef get_new_access_token():\n    \"\"\"\n    Usando el RefreshToken, obtenemos un access token\n    \"\"\"\n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n\n    auth_header = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n\n    headers = {\n        'Accept': 'application/json',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Authorization': f'Basic {auth_header}'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n    \n    # Endpoint de Sandbox\n    url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    \n    response = requests.post(url, headers=headers, data=data)\n    \n    if response.status_code != 200:\n        raise Exception(f\"Error renovando token: {response.text}\")\n        \n    tokens_dict = response.json()\n    \n    new_refresh_token = tokens_dict.get('refresh_token')\n    \n    if new_refresh_token and new_refresh_token != refresh_token:\n        print(f\"\u26a0\ufe0f AVISO DE ROTACI\u00d3N: Intuit ha entregado un nuevo Refresh Token.\")\n        print(f\"Nuevo Token para actualizar en Secrets: {new_refresh_token}\")\n    \n    # 4. Retornamos el access token como siempre\n    return tokens_dict['access_token']\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Extrae Invoices de QBO usando paginaci\u00f3n y filtros de fecha.\n    \"\"\"\n    # 1. Obtener Access Token fresco\n    access_token = get_new_access_token()\n    realm_id = get_secret_value('QBO_REALM_ID')\n    \n    # 2. Configurar Entorno (Sandbox)\n    base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}/query\"\n    \n    # 3. Leer Par\u00e1metros del Pipeline\n    # Formato esperado: 'YYYY-MM-DD'\n    start_date = kwargs['fecha_inicio']\n    end_date = kwargs['fecha_fin']\n    \n    print(f\"--- Iniciando extracci\u00f3n de Invoices desde {start_date} hasta {end_date} ---\")\n\n    all_invoices = []\n    start_position = 1\n    max_results = 100 # M\u00e1ximo permitido por QBO es 1000, usamos 100 para probar paginaci\u00f3n segura\n    has_more_data = True\n    \n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'application/text' # QBO Query usa text/plain\n    }\n\n    # 4. Bucle de Paginaci\u00f3n\n    while has_more_data:\n        # Query SQL de QBO (Filtro por MetaData.LastUpdatedTime)\n        # Nota: Convertimos las fechas a formato ISO simple para la query\n        query = (\n            f\"SELECT * FROM Invoice \"\n            f\"WHERE MetaData.LastUpdatedTime >= '{start_date}T00:00:00-05:00' \"\n            f\"AND MetaData.LastUpdatedTime <= '{end_date}T23:59:59-05:00' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n        )\n        \n        response = requests.post(base_url, headers=headers, data=query)\n        \n        if response.status_code == 429:\n            print(\"Rate Limit alcanzado. Esperando 5 segundos...\")\n            time.sleep(5)\n            continue\n            \n        if response.status_code != 200:\n            raise Exception(f\"Error en API QBO: {response.text}\")\n            \n        data = response.json()\n        query_response = data.get('QueryResponse', {})\n        invoices = query_response.get('Invoice', [])\n        \n        if not invoices:\n            print(\"No se encontraron m\u00e1s registros.\")\n            has_more_data = False\n        else:\n            print(f\"Pagina extraida: {len(invoices)} invoices. StartPos: {start_position}\")\n            \n            # 5. Transformaci\u00f3n ligera para capa RAW\n            # Preparamos la estructura que ir\u00e1 a Postgres\n            for inv in invoices:\n                row = {\n                    'id': inv['Id'], # PK requerida\n                    'payload': json.dumps(inv), # JSON completo requerido\n                    'ingested_at_utc': datetime.utcnow().isoformat(), # Metadata requerida\n                    'extract_window_start_utc': start_date,\n                    'extract_window_end_utc': end_date,\n                    'page_number': (start_position // max_results) + 1\n                }\n                all_invoices.append(row)\n            \n            start_position += len(invoices)\n            \n            # Si devolvi\u00f3 menos del m\u00e1ximo, es la \u00faltima p\u00e1gina\n            if len(invoices) < max_results:\n                has_more_data = False\n\n    # Retornamos un DataFrame (formato nativo de Mage)\n    print(f\"Extracci\u00f3n total: {len(all_invoices)} registros.\")\n    return pd.DataFrame(all_invoices)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/extract_invoices.py", "language": "python", "type": "data_loader", "uuid": "extract_invoices"}, "data_loaders/extract_items.py:data_loader:python:extract items": {"content": "import io\nimport pandas as pd\nimport requests\nimport json\nimport base64\nimport time\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\ndef get_new_access_token():\n    # --- LOGICA DE ROTACION Y AUTH (IGUAL QUE ANTES) ---\n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n\n    auth_header = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n\n    headers = {\n        'Accept': 'application/json',\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Authorization': f'Basic {auth_header}'\n    }\n    \n    data = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    \n    response = requests.post(url, headers=headers, data=data)\n    \n    if response.status_code != 200:\n        raise Exception(f\"Error renovando token: {response.text}\")\n        \n    tokens_dict = response.json()\n    new_refresh_token = tokens_dict.get('refresh_token')\n    \n    # Chequeo de rotaci\u00f3n\n    if new_refresh_token and new_refresh_token != refresh_token:\n        print(f\"\u26a0\ufe0f AVISO DE ROTACI\u00d3N: Nuevo Refresh Token detectado: {new_refresh_token}\")\n    \n    return tokens_dict['access_token']\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Extrae Items de QBO.\n    \"\"\"\n    access_token = get_new_access_token()\n    realm_id = get_secret_value('QBO_REALM_ID')\n    base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}/query\"\n    \n    start_date = kwargs['fecha_inicio'] \n    end_date = kwargs['fecha_fin']\n    \n    print(f\"--- Extrayendo ITEMS desde {start_date} hasta {end_date} ---\")\n\n    all_data = []\n    start_position = 1\n    max_results = 100\n    has_more_data = True\n    \n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'application/text'\n    }\n\n    while has_more_data:\n        # --- CAMBIO CLAVE: Query a la tabla Item ---\n        query = (\n            f\"SELECT * FROM Item \"\n            f\"WHERE MetaData.LastUpdatedTime >= '{start_date}T00:00:00-05:00' \"\n            f\"AND MetaData.LastUpdatedTime <= '{end_date}T23:59:59-05:00' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n        )\n        \n        response = requests.post(base_url, headers=headers, data=query)\n        \n        if response.status_code == 429:\n            print(\"Rate Limit. Esperando 5s...\")\n            time.sleep(5)\n            continue\n            \n        if response.status_code != 200:\n            raise Exception(f\"Error API: {response.text}\")\n            \n        data = response.json()\n        query_response = data.get('QueryResponse', {})\n        # --- CAMBIO CLAVE: La llave ahora es 'Item' ---\n        records = query_response.get('Item', [])\n        \n        if not records:\n            has_more_data = False\n        else:\n            print(f\"Pagina extraida: {len(records)} Items.\")\n            \n            for item in records:\n                row = {\n                    'id': item['Id'],\n                    'payload': json.dumps(item),\n                    'ingested_at_utc': datetime.utcnow().isoformat(),\n                    'extract_window_start_utc': start_date,\n                    'extract_window_end_utc': end_date,\n                    'page_number': (start_position // max_results) + 1\n                }\n                all_data.append(row)\n            \n            start_position += len(records)\n            if len(records) < max_results:\n                has_more_data = False\n\n    print(f\"Total Items: {len(all_data)}\")\n    return pd.DataFrame(all_data)\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/extract_items.py", "language": "python", "type": "data_loader", "uuid": "extract_items"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/qb_customers_backfill/metadata.yaml:pipeline:yaml:qb customers backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_customers\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extract_customers\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: extract_customers\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_customers\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - extract_customers\n  uuid: export_customers\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-01-29 00:46:53.556752+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_customers_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_customers_backfill\nvariables:\n  fecha_fin: '2025-12-31'\n  fecha_inicio: '2014-01-01'\nvariables_dir: /home/src/mage_data/qbo_project\nwidgets: []\n", "file_path": "pipelines/qb_customers_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_customers_backfill/metadata"}, "pipelines/qb_customers_backfill/__init__.py:pipeline:python:qb customers backfill/  init  ": {"content": "", "file_path": "pipelines/qb_customers_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_customers_backfill/__init__"}, "pipelines/qb_invoices_backfill/metadata.yaml:pipeline:yaml:qb invoices backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_invoices\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extract_invoices\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: extract_invoices\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_invoices\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - extract_invoices\n  uuid: export_invoices\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-01-28 23:07:23.578500+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_backfill\nvariables:\n  fecha_fin: '2025-12-31'\n  fecha_inicio: '2014-01-01'\nvariables_dir: /home/src/mage_data/qbo_project\nwidgets: []\n", "file_path": "pipelines/qb_invoices_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/metadata"}, "pipelines/qb_invoices_backfill/__init__.py:pipeline:python:qb invoices backfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_backfill/__init__"}, "pipelines/qb_items_backfill/metadata.yaml:pipeline:yaml:qb items backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/extract_items.py\n    file_source:\n      path: data_loaders/extract_items.py\n  downstream_blocks:\n  - export_items\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extract_items\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: extract_items\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_items\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - extract_items\n  uuid: export_items\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-01-29 00:46:53.556752+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_items_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_items_backfill\nvariables:\n  fecha_fin: '2025-12-31'\n  fecha_inicio: '2014-01-01'\nvariables_dir: /home/src/mage_data/qbo_project\nwidgets: []\n", "file_path": "pipelines/qb_items_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_items_backfill/metadata"}, "pipelines/qb_items_backfill/__init__.py:pipeline:python:qb items backfill/  init  ": {"content": "", "file_path": "pipelines/qb_items_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_items_backfill/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}